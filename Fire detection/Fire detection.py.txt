# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.decomposition import PCA
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Conv2D, Flatten, TimeDistributed, Dropout, BatchNormalization, Reshape
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_squared_error
import os
# Load training data
train_data = pd.read_csv('/content/train.csv')
test_data = pd.read_csv('/content/test.csv')

# Display the data
print("Training Data Head:\n", train_data.head())
print("Test Data Head:\n", test_data.head())

# Check for missing values
print("Missing values in training data:\n", train_data.isnull().sum())
print("Missing values in test data:\n", test_data.isnull().sum())

# Select numeric columns
numeric_cols = train_data.select_dtypes(include=np.number).columns.tolist()
print(f'Numeric columns: {numeric_cols}')

# Normalize numeric columns
scaler = MinMaxScaler()
train_data[numeric_cols] = scaler.fit_transform(train_data[numeric_cols])
test_data[numeric_cols] = scaler.transform(test_data[numeric_cols])

# Handle missing values
train_data[numeric_cols] = train_data[numeric_cols].fillna(train_data[numeric_cols].mean())
test_data[numeric_cols] = test_data[numeric_cols].fillna(test_data[numeric_cols].mean())

# Basic descriptive analysis
print("Descriptive Statistics for Training Data:\n", train_data.describe())
sns.pairplot(train_data[numeric_cols])
plt.show()

# Feature interaction
for col1 in numeric_cols:
    for col2 in numeric_cols:
        if col1 != col2:
            train_data[f'{col1}_x_{col2}'] = train_data[col1] * train_data[col2]
            test_data[f'{col1}_x_{col2}'] = test_data[col1] * test_data[col2]

# Split data into features and targets
X = train_data.drop(columns=['xi_filename'])  
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(train_data['xi_filename'])

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Use only numeric columns for PCA
numeric_cols = X_train.select_dtypes(include=np.number).columns.tolist()
X_train_numeric = X_train[numeric_cols]
X_val_numeric = X_val[numeric_cols]
test_data_numeric = test_data[numeric_cols]

# PCA for dimensionality reduction
pca = PCA(n_components=0.95)  # Keep 95% of variance
X_train_pca = pca.fit_transform(X_train_numeric)
X_val_pca = pca.transform(X_val_numeric)
test_data_pca = pca.transform(test_data_numeric)
# Build a CNN-LSTM model
def create_model(input_shape):
    model = Sequential()
    model.add(Reshape((input_shape[0], input_shape[1], 1), input_shape=input_shape))  # Reshape for Conv2D
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))
    
    model.add(TimeDistributed(Flatten()))
    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(64, return_sequences=False))
    model.add(Dropout(0.3))
    
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(np.unique(y)), activation='softmax'))  # Use 'softmax' for multiclass
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Reshape the data for LSTM
X_train_reshaped = X_train_pca.reshape((X_train_pca.shape[0], 1, X_train_pca.shape[1]))
X_val_reshaped = X_val_pca.reshape((X_val_pca.shape[0], 1, X_val_pca.shape[1]))
input_shape = (X_train_reshaped.shape[1], X_train_reshaped.shape[2])

# Build the model
model = create_model(input_shape)
model.summary()

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model with early stopping
history = model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32,
                    validation_data=(X_val_reshaped, y_val),
                    callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(X_val_reshaped, y_val)
print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')

# Predict on validation data to compute MSE
val_predictions = model.predict(X_val_reshaped)
predicted_classes_val = np.argmax(val_predictions, axis=1)
mse = mean_squared_error(y_val, predicted_classes_val)
print(f'Validation Mean Squared Error: {mse}')

# Predict on test data
test_data_reshaped = test_data_pca.reshape((test_data_pca.shape[0], 1, test_data_pca.shape[1]))
predictions = model.predict(test_data_reshaped)
predicted_classes = np.argmax(predictions, axis=1)

# Prepare submission
submission = pd.DataFrame({'id': test_data['id'], 'Predictions': label_encoder.inverse_transform(predicted_classes)})
submission.to_csv('submission.csv', index=False)

# Visualize results
plt.figure(figsize=(10, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')
plt.show()
# Evaluate the model on the test data
def evaluate_model(model, test_data, scaler, pca):
    """
    Evaluate the model using the test dataset.
    
    Parameters:
    model (Keras model): Trained model
    test_data (DataFrame): DataFrame containing test features
    scaler (MinMaxScaler): Scaler used for normalization
    pca (PCA): PCA model used for dimensionality reduction
    
    Returns:
    mse (float): Mean Squared Error of predictions
    """
    # Preprocess test data
    test_data[numeric_cols] = scaler.transform(test_data[numeric_cols])
    test_data_pca = pca.transform(test_data[numeric_cols])
    test_data_reshaped = test_data_pca.reshape((test_data_pca.shape[0], 1, test_data_pca.shape[1]))

    # Predictions
    predictions = model.predict(test_data_reshaped)
    predicted_classes = np.argmax(predictions, axis=1)

    # Calculate MSE (assuming you have ground truth labels for the test set)
    mse = mean_squared_error(ground_truth_labels, predicted_classes)  # Make sure to define ground_truth_labels
    return mse

# Example usage
mse = evaluate_model(model, test_data, scaler, pca)
print(f'Test Mean Squared Error: {mse}')